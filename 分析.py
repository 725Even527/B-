import os
import pandas as pd
import jieba
from collections import Counter
import itertools
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from wordcloud import WordCloud
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

# === è®¾ç½®è·¯å¾„ ===
base_dir = os.path.dirname(__file__)  # å½“å‰è„šæœ¬è·¯å¾„
data_path = os.path.join(base_dir, '../output/combined_danmakus_all.xlsx')  # ç›¸å¯¹è·¯å¾„è¯»å–
output_path = os.path.join(base_dir, 'åˆ†è¯ç»“æœ.xlsx')

# === 1. è¯»å–æ•°æ®ï¼ˆXLSXï¼‰ ===
df = pd.read_excel(data_path)
df = df[['å†…å®¹']].dropna().drop_duplicates()
df = df[df['å†…å®¹'].astype(str).str.len() >= 4].reset_index(drop=True)
print(f"âœ… è¯»å–å¹¶æ¸…æ´—æ•°æ®ï¼Œå…± {len(df)} æ¡å¼¹å¹•")

# === 2. åŠ è½½ç”¨æˆ·è¯å…¸ä¸åœç”¨è¯ ===
jieba.load_userdict('./stop_dict/keep_words.txt')
stopwords = set()
for fname in ['./stop_dict/stopwords_hit.txt', './stop_dict/mystopwords.txt']:
    with open(fname, encoding='utf-8') as f:
        stopwords.update(f.read().splitlines())
stopwords.update([' ', '', '\n', '\t', '\xa0', '\u3000', 'ï¿½'])

# === 3. åˆ†è¯ ===
def get_cut_content(text):
    words = jieba.cut(text, cut_all=False, HMM=True)
    return [w for w in words if w not in stopwords and not w.isdigit() and len(w) > 1]

df['åˆ†è¯'] = df['å†…å®¹'].astype(str).map(get_cut_content)
df['åˆ†è¯æ–‡æœ¬'] = df['åˆ†è¯'].map(lambda x: ' '.join(x))
df[['åˆ†è¯æ–‡æœ¬']].to_excel(output_path, index=False)
print("âœ… å·²ä¿å­˜åˆ†è¯ç»“æœ")

# === 4. TF-IDF å…³é”®è¯æå– ===
import jieba.analyse as analyse
tfidf_tags = analyse.extract_tags(' '.join(df['åˆ†è¯æ–‡æœ¬']), topK=50, withWeight=True)
tfidf_df = pd.DataFrame(tfidf_tags, columns=['word', 'tfidf'])
tfidf_df.to_csv('TF-IDFå…³é”®è¯.csv', index=False, encoding='utf-8-sig')
print("âœ… å·²ä¿å­˜ TF-IDF å…³é”®è¯")

# === 5. è¯é¢‘ç»Ÿè®¡ ===
all_words = list(itertools.chain(*df['åˆ†è¯']))
word_counts = Counter(all_words)
word_counts_df = pd.DataFrame(word_counts.items(), columns=['word', 'count']).sort_values(by='count', ascending=False)
word_counts_df.to_csv('è¯é¢‘ç»Ÿè®¡.csv', index=False, encoding='utf-8-sig')
top_words_df = word_counts_df.head(20)
top_words_df.to_csv('è¯é¢‘Top20.csv', index=False)
print("âœ… å·²ä¿å­˜è¯é¢‘ç»Ÿè®¡å’ŒTop20")

# === 6. è¯é¢‘æŸ±çŠ¶å›¾ ===
plt.rcParams["font.sans-serif"] = ['SimHei']
plt.rcParams["axes.unicode_minus"] = False
plt.figure(figsize=(12, 8))
plt.title('é«˜é¢‘è¯ç»Ÿè®¡')
plt.bar(top_words_df['word'], top_words_df['count'])
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('é«˜é¢‘è¯æŸ±çŠ¶å›¾.png')
plt.show()
print("âœ… å·²ä¿å­˜é«˜é¢‘è¯æŸ±çŠ¶å›¾")

# === 7. è¯äº‘å›¾ ===
wc = WordCloud(
    background_color='white',
    font_path='C:/Windows/Fonts/simhei.ttf',
    width=1000,
    height=800,
    max_words=300,
    max_font_size=100,
    scale=2
).generate_from_frequencies(word_counts)

plt.figure(figsize=(12, 8))
plt.imshow(wc, interpolation="bilinear")
plt.axis('off')
plt.tight_layout()
plt.savefig('è¯äº‘å›¾.jpg')
plt.show()
print("âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜è¯äº‘å›¾")

# === 8. æƒ…æ„Ÿåˆ†æ ===
try:
    print("ğŸ“¦ æ­£åœ¨åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
    model_name = "uer/roberta-base-finetuned-jd-binary-chinese"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def predict_sentiment(text):
        if not isinstance(text, str) or text.strip() == "":
            return None, None
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128, padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]
        return ("positive" if probs[1] >= probs[0] else "negative", float(probs.max()))

    df['æƒ…æ„Ÿæ ‡ç­¾'] = ''
    df['æƒ…æ„Ÿç½®ä¿¡åº¦'] = 0.0
    for idx, row in df.iterrows():
        label, score = predict_sentiment(row['å†…å®¹'])
        df.at[idx, 'æƒ…æ„Ÿæ ‡ç­¾'] = label
        df.at[idx, 'æƒ…æ„Ÿç½®ä¿¡åº¦'] = score

    df.to_csv("å¼¹å¹•æƒ…æ„Ÿåˆ†æç»“æœ.csv", index=False, encoding='utf-8-sig')
    print("âœ… æƒ…æ„Ÿåˆ†æå·²å®Œæˆå¹¶ä¿å­˜ä¸º å¼¹å¹•æƒ…æ„Ÿåˆ†æç»“æœ.csv")
except Exception as e:
    print(f"âŒ æƒ…æ„Ÿåˆ†æå¤±è´¥ï¼š{e}")

# === 9. è¾“å‡ºç”Ÿæˆçš„æ–‡ä»¶ ===
print("\nğŸ“ å½“å‰ç›®å½•ä¸‹ç”Ÿæˆçš„ç»“æœæ–‡ä»¶ï¼š")
for f in os.listdir():
    if f.endswith(('.csv', '.xlsx', '.png', '.jpg')):
        print(" -", f)
